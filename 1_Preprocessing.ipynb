{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that we want to use\n",
    "columns = ['MET', 'PRA', 'ASE', 'EVA', 'RET', 'APR', 'DOM', 'REC', 'MEJ', 'GÉNERO ALUMNO', 'PROM ACUMULADO EN PROFESIONAL', 'Género del profesor', 'Por qué no lo recomendarias', 'Comentarios']\n",
    "aspects = ['MET', 'PRA', 'ASE', 'EVA', 'RET', 'APR', 'DOM', 'REC', 'MEJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drive.mount('/content/drive')\n",
    "# df=pd.read_excel('drive/MyDrive/Tesis/ecoa-demografico.xlsx')\n",
    "df = pd.read_excel('ecoa-demografico.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Our original dataset is of shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecoas = df[columns]\n",
    "df_ecoas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None # Disable warning\n",
    "df_ecoas = df_ecoas.dropna(subset=['Por qué no lo recomendarias', 'Comentarios'], thresh=1) # Delete all rows where there are neither negative or positive comments\n",
    "df_ecoas['GÉNERO ALUMNO'] = np.where((df_ecoas['GÉNERO ALUMNO'] == 'Femenino'), 0, df_ecoas['GÉNERO ALUMNO'])\n",
    "df_ecoas['GÉNERO ALUMNO'] = np.where((df_ecoas['GÉNERO ALUMNO'] == 'Masculino'), 1, df_ecoas['GÉNERO ALUMNO'])\n",
    "df_ecoas['Género del profesor'] = np.where((df_ecoas['Género del profesor'] == 'Femenino'), 0, df_ecoas['Género del profesor'])\n",
    "df_ecoas['Género del profesor'] = np.where((df_ecoas['Género del profesor'] == 'Masculino'), 1, df_ecoas['Género del profesor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for asp in aspects: # We delete all incomplete submissions\n",
    "  df_ecoas.loc[df_ecoas[asp].isnull(),asp] = 99\n",
    "  df_ecoas = df_ecoas.loc[df_ecoas[asp] != 99]\n",
    "df_ecoas = df_ecoas.reset_index(drop=True) # Reset the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecoas= df_ecoas.rename(columns={\"Comentarios\": \"Positivos\"})\n",
    "df_ecoas= df_ecoas.rename(columns={\"Por qué no lo recomendarias\": \"Negativos\"})\n",
    "p_only = df_ecoas['Positivos'].count()\n",
    "n_only = df_ecoas['Negativos'].count()\n",
    "print(f'Tenemos { p_only } comentarios SOLO positivos')\n",
    "print(f'Tenemos { n_only } comentarios SOLO negativos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecoas['Tipo Comentario'] = 0\n",
    "df_ecoas.loc[pd.notna(df_ecoas['Positivos']), 'Tipo Comentario'] = 1\n",
    "df_ecoas.loc[pd.notna(df_ecoas['Negativos']) & pd.notna(df_ecoas['Positivos']), 'Tipo Comentario'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecoas['Negativos'] = df_ecoas['Negativos'].fillna('')\n",
    "df_ecoas['Positivos'] = df_ecoas['Positivos'].fillna('')\n",
    "df_ecoas[\"Comentarios\"] = df_ecoas[\"Positivos\"] + ' ' + df_ecoas[\"Negativos\"]\n",
    "df_ecoas = df_ecoas.drop(columns=['Negativos', 'Positivos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecoas['AVG'] = df_ecoas[['APR', 'EVA', 'MET', 'PRA', 'REC', 'RET']].mean(axis=1).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the data set after initial cleaning\n",
    "df_ecoas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "*In this step we performed some NLP preprocessing techquines such as stop words removal, symbols and special characters removal and lemmatization of the entire comments column*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df_ecoas['Comentarios']\n",
    "\n",
    "for a in range(len(comments)):\n",
    "  if type(comments[a]) is not str:\n",
    "    print(type(comments[a]), 'No es str: ', a)\n",
    "    df_ecoas.drop(index=a)\n",
    "\n",
    "comments = comments.str.lower()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('spanish')\n",
    "stop.remove('no')\n",
    "stop.append('es')\n",
    "\n",
    "# Tokenize the comments\n",
    "for i in range(len(comments)):\n",
    "  comments[i] = str(comments[i]).split()\n",
    "  comments[i] = [word for word in comments[i] if word not in stop]\n",
    "\n",
    "# Remove the symbols and replace them with blank spaces\n",
    "sim = \"!\\\"#$%&()*+-.,/:;<=>?@[\\]^_`{|}~\\n\"\n",
    "\n",
    "# Para cada símbolo, se remplaza por blank space\n",
    "for c in range(len(comments)):\n",
    "  for i in range(len(comments[c])):\n",
    "    for j in sim:\n",
    "      if comments[c][i].__contains__(j):\n",
    "        comments[c][i] = comments[c][i].replace(j,'')\n",
    "\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_less2  = 0\n",
    "total_si = 0\n",
    "total_no = 0\n",
    "\n",
    "for comm in comments:\n",
    "  if len(comm) <2:\n",
    "    total_less2 += 1\n",
    "    if 'si' in comm:\n",
    "      total_si += 1\n",
    "    if 'no' in comm:\n",
    "      total_no += 1\n",
    "\n",
    "print(f'Tenemos {total_less2} comentarios con una sola palabra, sin embargo, no los eliminaremos porque muchos son palabras que pueden ayudar al modelo')\n",
    "print(f'Tenemos {total_si} comentarios con la palabra \"si\"')\n",
    "print(f'Tenemos {total_no} comentarios con la palabra \"no\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = list(comments)\n",
    "for i in range(len(comments)):\n",
    "  while '' in comments[i]:\n",
    "    comments[i].remove('')\n",
    "\n",
    "comentarios_merge=[None]*len(comments)\n",
    "\n",
    "for i in range(len(comentarios_merge)):\n",
    "  comentarios_merge[i]=(TreebankWordDetokenizer().detokenize(comments[i]))\n",
    "\n",
    "print('Example of comment:')\n",
    "comentarios_merge[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_lemm = []\n",
    "\n",
    "# Config the Stanza pipeline\n",
    "config = {\n",
    "\t'processors': 'tokenize,mwt,pos,lemma',\n",
    "  'lang': 'es',\n",
    "}\n",
    "nlp = stanza.Pipeline(**config) # Initialize the pipeline using a configuration dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "com_lemmatized = []\n",
    "comments_lemm = []\n",
    "\n",
    "def lemmatize_comments(comment):\n",
    "  doc = nlp(comment)\n",
    "  lemmatized_comment = []\n",
    "\n",
    "  for sent in doc.sentences:\n",
    "    lemmatized_comment.extend([word.lemma for word in sent.words])\n",
    "  return lemmatized_comment\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "  results = executor.map(lemmatize_comments, comentarios_merge)\n",
    "\n",
    "comments_lemm = list(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Example of comment after lemmatizaion:')\n",
    "comments_lemm[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecoas['Lemm'] = comments_lemm\n",
    "df_ecoas.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecoas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecoas.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecoas.to_csv('./datasets/df_ready.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
